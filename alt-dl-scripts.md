# Alternative Download scripts

In the download process, multiple files were corrupted. In future need to use better download practies. Below are scripts generated by perplexity that need to be checked.

```bash
# Download all files
urls=(
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/008/SRR1039508/SRR1039508_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/008/SRR1039508/SRR1039508_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/009/SRR1039509/SRR1039509_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/009/SRR1039509/SRR1039509_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/000/SRR1039510/SRR1039510_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/000/SRR1039510/SRR1039510_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/001/SRR1039511/SRR1039511_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/001/SRR1039511/SRR1039511_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/002/SRR1039512/SRR1039512_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/002/SRR1039512/SRR1039512_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/003/SRR1039513/SRR1039513_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/003/SRR1039513/SRR1039513_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/004/SRR1039514/SRR1039514_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/004/SRR1039514/SRR1039514_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/005/SRR1039515/SRR1039515_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/005/SRR1039515/SRR1039515_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/006/SRR1039516/SRR1039516_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/006/SRR1039516/SRR1039516_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/007/SRR1039517/SRR1039517_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/007/SRR1039517/SRR1039517_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/008/SRR1039518/SRR1039518_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/008/SRR1039518/SRR1039518_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/009/SRR1039519/SRR1039519_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/009/SRR1039519/SRR1039519_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/000/SRR1039520/SRR1039520_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/000/SRR1039520/SRR1039520_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/001/SRR1039521/SRR1039521_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/001/SRR1039521/SRR1039521_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/002/SRR1039522/SRR1039522_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/002/SRR1039522/SRR1039522_2.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/003/SRR1039523/SRR1039523_1.fastq.gz"
    "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/003/SRR1039523/SRR1039523_2.fastq.gz"
)

# Download each file
failed_downloads=()
for url in "${urls[@]}"; do
    if ! download_and_verify "$url"; then
        failed_downloads+=("$url")
    fi
done

# Summary
echo "Download completed at $(date)" | tee -a $LOGFILE
if [ ${#failed_downloads[@]} -eq 0 ]; then
    echo "All downloads successful!" | tee -a $LOGFILE
else
    echo "Failed downloads:" | tee -a $LOGFILE
    for failed in "${failed_downloads[@]}"; do
        echo "  $failed" | tee -a $LOGFILE
    done
fi
```

Another chat of Perplexity criticised the above code and suggested the following code instead. This is a reminder not to fully trust or believe the AI output, we must check it first.

```bash

#!/usr/bin/env bash
set -euo pipefail

# Log and control settings
LOG="download_$(date -Iseconds).log"
MAX_TRIES=5
PARALLEL_JOBS=8

# ←── Add your URLs here ──→
urls=(
  "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/008/SRR1039508/SRR1039508_1.fastq.gz"
  "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/008/SRR1039508/SRR1039508_2.fastq.gz"
  # …add all other URLs in this block…
)

log() { echo "[$(date -Iseconds)] [$1] $2" | tee -a "$LOG"; }

download_verify() {
  local url="$1" file="$(basename "$url")"
  for ((i=1; i<=MAX_TRIES; i++)); do
    log INFO "Attempt $i: Downloading $file"
    if wget --timeout=30 --dns-timeout=10 --connect-timeout=15 \
             --read-timeout=60 --tries=1 --retry-connrefused \
             -O "$file" "$url" 2>>"$LOG"; then
      if gzip -t "$file" && [[ $(stat -c%s "$file") -ge 1000 ]]; then
        log INFO "Verified gzip and size: $file"
        return 0
      fi
    fi
    log WARN "Attempt $i failed for $file"
    rm -f "$file"
    sleep $((i * 5))
  done
  log ERROR "Failed after $MAX_TRIES attempts: $file"
  return 1
}

export -f download_verify log MAX_TRIES LOG
printf '%s\n' "${urls[@]}" | parallel -j"$PARALLEL_JOBS" download_verify
```